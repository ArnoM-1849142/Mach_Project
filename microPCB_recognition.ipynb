{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "detailed-marijuana",
   "metadata": {},
   "source": [
    "# Machine learning project  \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this project, an implementation of the backpropagation algorithm for neural networks is made and applied to the task of [`micro_PCB`](https://www.kaggle.com/frettapper/micropcb-images) recognition. This project is made for the machine learning course [`Machine learning`](https://uhintra03.uhasselt.be/studiegidswww/opleidingsonderdeel.aspx?a=2021&i=4483&n=4&t=04) given at the joint training of kuleuven and uhasselt. The authors of this project are Molenaers Arno and Purnal Lennert.\n",
    "\n",
    "\n",
    "The libraries that need to be imported for this project are the following:\n",
    "- [`numpy`](http://www.numpy.org/) for all arrays and matrix operations.\n",
    "- [`matplotlib`](https://matplotlib.org/) for plotting.\n",
    "- [`scipy`](https://docs.scipy.org/doc/scipy/reference/) for scientific and numerical computation functions and tools.\n",
    "- [`csv`](https://docs.python.org/3/library/csv.html) for importing the image data\n",
    "- [`utils`]() utilities from exercise 4 from the coursera machine learning course\n",
    "- [`math`](https://docs.python.org/3/library/math.html) math library used for functions like square root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "advanced-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# math library \n",
    "import math\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# Used for imorting csv data\n",
    "import csv\n",
    "\n",
    "# utilies library from the exercises of the machine learning course\n",
    "import utils\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-innocent",
   "metadata": {},
   "source": [
    "## importing the data\n",
    "the training data is imported from a csv file. The X matrix contains the input features as a '6500x7500' matrix. y is a matrix containing the labels, for the neural network each label is encoded as a 13 dimensional vector with a 1 at the corresponding correct label and the other elements set to 0, making y a '6500x13' vector.\n",
    "> do not forget to specify the correct path to the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "elder-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((6500,7500))\n",
    "y = np.zeros((6500,13))\n",
    "alpha = ['A','B','C','D','E','F','G','H','I','J','K','L','M']\n",
    "\n",
    "with open('../Channeldata.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        X[i,:] = row[1:]\n",
    "        # find the position of the label in the alphabet\n",
    "        j = alpha.index(row[0][0])\n",
    "        # set the label to 1 at that position\n",
    "        y[i,j] = 1\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-overhead",
   "metadata": {},
   "source": [
    "## cost function \n",
    "in the next block the cost function for this dataset is initialized. this is done using the following cost function for neural networks:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right] + \\frac{\\lambda}{2 m} \\left[ \\sum_{j=1}^{sl+1} \\sum_{k=1}^{sl} \\left( \\Theta_{j,k}^{(1)} \\right)^2 + \\sum_{j=1}^{sl+1} \\sum_{k=1}^{sl} \\left( \\Theta_{j,k}^{(2)} \\right)^2 \\right] $$\n",
    "\n",
    "we also first initialize the sigmoidgradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "present-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the sigmoid function evaluated at z. \n",
    "    This should work regardless if z is a matrix or a vector. \n",
    "    In particular, if z is a vector or matrix, you should return\n",
    "    the gradient for each element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        A vector or matrix as input to the sigmoid function. \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    g : array_like\n",
    "        Gradient of the sigmoid function. Has the same shape as z. \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    g = utils.sigmoid(z)*(1-utils.sigmoid(z))\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rising-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0):\n",
    "    \"\"\"\n",
    "    Implements the neural network cost function and gradient for a two layer neural \n",
    "    network which performs classification. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_params : array_like\n",
    "        The parameters for the neural network which are \"unrolled\" into \n",
    "        a vector. This needs to be converted back into the weight matrices Theta1\n",
    "        and Theta2.\n",
    "    \n",
    "    input_layer_size : int\n",
    "        Number of features for the input layer. \n",
    "    \n",
    "    hidden_layer_size : int\n",
    "        Number of hidden units in the second layer.\n",
    "    \n",
    "    num_labels : int\n",
    "        Total number of labels, or equivalently number of units in output layer. \n",
    "    \n",
    "    X : array_like\n",
    "        Input dataset. A matrix of shape (m x input_layer_size).\n",
    "    \n",
    "    y : array_like\n",
    "        Dataset labels. A vector of shape (m,num_labels).\n",
    "    \n",
    "    lambda_ : float, optional\n",
    "        Regularization parameter.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the cost function at the current weight values.\n",
    "    \n",
    "    grad : array_like\n",
    "        An \"unrolled\" vector of the partial derivatives of the concatenatation of\n",
    "        neural network weights Theta1 and Theta2.\n",
    "    \n",
    "    Note \n",
    "    ----\n",
    "    We have provided an implementation for the sigmoid function in the file \n",
    "    `utils.py` accompanying this assignment.\n",
    "    \"\"\"\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))\n",
    "\n",
    "    # Setup some useful variables\n",
    "    m = y.shape[0]\n",
    "         \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "\n",
    "    # part 1: feedforward   \n",
    "    # adding column of 1's to X (bias terms)\n",
    "    a1 = np.concatenate([np.ones((m,1)),X], axis=1)\n",
    "    \n",
    "    z2 = np.dot(a1, Theta1.T)\n",
    "    a2 = utils.sigmoid(z2)\n",
    "    a2 = np.concatenate([np.ones((a2.shape[0], 1)), a2], axis=1)\n",
    "    \n",
    "    z3 = np.dot(a2, Theta2.T)\n",
    "    h = utils.sigmoid(z3)\n",
    "                         \n",
    "    J = 1/m * np.sum(-y*np.log(h)-(1-y)*np.log(1-h))\n",
    "    # add regularization\n",
    "    J = J + (lambda_ / (2*m)) * (np.sum(Theta1[:, 1:]**2) +np.sum(Theta2[:, 1:]**2))\n",
    "    \n",
    "    # part 2: backpropagation\n",
    "    delta3 = h - y\n",
    "    delta2 = np.dot(delta3, Theta2)[:, 1:] * sigmoidGradient(z2)\n",
    "    \n",
    "    DELTA1 = np.dot(delta2.T, a1)\n",
    "    DELTA2 = np.dot(delta3.T, a2)\n",
    "    \n",
    "    \n",
    "    Theta1_grad = 1/m * DELTA1\n",
    "    Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (lambda_ / m) * Theta1[:, 1:]\n",
    "    Theta2_grad = 1/m * DELTA2\n",
    "    Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (lambda_ / m) * Theta2[:, 1:]\n",
    "    \n",
    "    # Unroll gradients\n",
    "    # grad = np.concatenate([Theta1_grad.ravel(order=order), Theta2_grad.ravel(order=order)])\n",
    "    grad = np.concatenate([Theta1_grad.ravel(), Theta2_grad.ravel()])\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-prefix",
   "metadata": {},
   "source": [
    "## initializing the theta parameters\n",
    "This must be done in a random way to avoid symmetry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "boxed-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randInitializeWeights(L_in, L_out, epsilon_init=0.12):\n",
    "    \"\"\"\n",
    "    Randomly initialize the weights of a layer in a neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    L_in : int\n",
    "        Number of incomming connections.\n",
    "    \n",
    "    L_out : int\n",
    "        Number of outgoing connections. \n",
    "    \n",
    "    epsilon_init : float, optional\n",
    "        Range of values which the weight can take from a uniform \n",
    "        distribution.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    W : array_like\n",
    "        The weight initialiatized to random values.  Note that W should\n",
    "        be set to a matrix of size(L_out, 1 + L_in) as\n",
    "        the first column of W handles the \"bias\" terms.\n",
    "    \"\"\"\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    W = np.zeros((L_out, 1 + L_in))\n",
    "\n",
    "    W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-studio",
   "metadata": {},
   "source": [
    "The above function requires us to pass the layer sizes. These are initialized below and the function is then executed for each layer. The theta vectors for each layer are then combined and unrolled.\n",
    "The value for $\\epsilon_{init}$ is chosen with the following function.\n",
    "\n",
    "$$\\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in} + L_{out}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pressing-surname",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon init 1 = 0.019999999999999997\n",
      "epsilon init 2 = 0.02825979003336604\n"
     ]
    }
   ],
   "source": [
    "input_layer_size = X.shape[1]\n",
    "hidden_layer_size = 7500\n",
    "num_labels = y.shape[1]\n",
    "\n",
    "eps1 = math.sqrt(6)/math.sqrt(input_layer_size + hidden_layer_size)\n",
    "eps2 = math.sqrt(6)/math.sqrt(hidden_layer_size + num_labels)\n",
    "print('epsilon init 1 = ' + str(eps1))\n",
    "print('epsilon init 2 = ' + str(eps2))\n",
    "\n",
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size, eps1)\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels, eps2)\n",
    "\n",
    "initial_nn_params = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-attachment",
   "metadata": {},
   "source": [
    "## learing the parameters with `scipy.optimize.minimize`\n",
    "next we will use the scipy.optimize.minimize function to minimize the randomly initialized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "monthly-system",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\devWorks\\MACH\\MicroPCBs\\Mach_Project\\utils.py:179: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "# we initialize the minimize function options\n",
    "options = {'maxiter':100}\n",
    "\n",
    "#choose a lambda\n",
    "lambda_ = 1\n",
    "\n",
    "#creating a lambda function for the cost function\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "\n",
    "# execute the optimization function\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution parameters\n",
    "nn_params = res.x\n",
    "\n",
    "#reshape nn_params to retrieve the seperate theta matrixes \n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-validity",
   "metadata": {},
   "source": [
    "## checking the cost function and the gradients\n",
    "next we will verify the outcome of the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "later-faith",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters: 1.946889\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 1\n",
    "J, grads = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                      num_labels, X, y, lambda_)\n",
    "\n",
    "print('Cost at parameters: %.6f' % J)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-rochester",
   "metadata": {},
   "source": [
    "## save the theta values to csv file\n",
    "after minimizing theta it might be useful to save the values to a csv. The naming of the output files is `nnParametersxLy.csv` where x is the number of layers and y is the number of nodes in the hidden layer(s)\n",
    "> do not forget to set the correct name/path for the output file as pleased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../nnParameters3L7500.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow(nn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "north-synthesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01269617  0.01514906 -0.00621697 ...  0.01981407 -0.02612224\n",
      " -0.00453004]\n"
     ]
    }
   ],
   "source": [
    "print(nn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

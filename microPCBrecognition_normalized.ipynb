{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "detailed-marijuana",
   "metadata": {},
   "source": [
    "# Machine learning project  \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this project, an implementation of the backpropagation algorithm for neural networks is made and applied to the task of [`micro_PCB`](https://www.kaggle.com/frettapper/micropcb-images) recognition. This project is made for the machine learning course [`Machine learning`](https://uhintra03.uhasselt.be/studiegidswww/opleidingsonderdeel.aspx?a=2021&i=4483&n=4&t=04) given at the joint training of kuleuven and uhasselt. The authors of this project are Molenaers Arno and Purnal Lennert.\n",
    "\n",
    "\n",
    "The libraries that need to be imported for this project are the following:\n",
    "- [`numpy`](http://www.numpy.org/) for all arrays and matrix operations.\n",
    "- [`matplotlib`](https://matplotlib.org/) for plotting.\n",
    "- [`scipy`](https://docs.scipy.org/doc/scipy/reference/) for scientific and numerical computation functions and tools.\n",
    "- [`csv`](https://docs.python.org/3/library/csv.html) for importing the image data\n",
    "- [`utils`]() utilities from exercise 4 from the coursera machine learning course\n",
    "- [`math`](https://docs.python.org/3/library/math.html) math library used for functions like square root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# math library \n",
    "import math\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# Used for imorting csv data\n",
    "import csv\n",
    "\n",
    "# utilies library from the exercises of the machine learning course\n",
    "import utils\n",
    "\n",
    "# custom made functions for this Machine Learning dataset\n",
    "import customUtils as cu\n",
    "\n",
    "#used for timing the optimizations\n",
    "from timeit import default_timer as timer \n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# used for importing/exporting matlab data\n",
    "from scipy.io import savemat\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-innocent",
   "metadata": {},
   "source": [
    "## importing the training data\n",
    "the training data is imported from a csv file. The X matrix contains the input features as a '6500x7500' matrix. y is a matrix containing the labels, for the neural network each label is encoded as a 13 dimensional vector with a 1 at the corresponding correct label and the other elements set to 0, making y a '6500x13' vector.\n",
    "> do not forget to specify the correct path to the csv file and to set the size of the imported data as well as the resolution of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = cu.importImageDataFromCSV('../channeldata/channeldata50x50train.csv', data_size=6500)\n",
    "# matlab function\n",
    "X, y = cu.importImageTrainDataFromMatlab('dataset.mat', data_size=6500)\n",
    "X = X / 255.0\n",
    "# matlab function\n",
    "#X, y = importImageTrainDataFromMatlab('dataset.mat', data_size=6500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test, y_test = cu.importImageDataFromCSV('../channeldata/channeldata50x50test.csv', data_size=1625)\n",
    "# matlab function\n",
    "X_test, y_test = cu.importImageTestDataFromMatlab('dataset.mat', data_size=1625)\n",
    "X_test = X_test / 255.0\n",
    "# matlab function\n",
    "#X_test, y_test = importImageTestDataFromMatlab('dataset.mat', data_size=1625)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-overhead",
   "metadata": {},
   "source": [
    "## cost function \n",
    "The cost funtion used is as follows:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[ - y_k^{(i)} \\log \\left( \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) - \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1 - \\left( h_\\theta \\left( x^{(i)} \\right) \\right)_k \\right) \\right] + \\frac{\\lambda}{2 m} \\left[ \\sum_{j=1}^{sl+1} \\sum_{k=1}^{sl} \\left( \\Theta_{j,k}^{(1)} \\right)^2 + \\sum_{j=1}^{sl+1} \\sum_{k=1}^{sl} \\left( \\Theta_{j,k}^{(2)} \\right)^2 \\right] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-prefix",
   "metadata": {},
   "source": [
    "## initializing the theta parameters and hyperparameters\n",
    "The theta parameters must be initialized in a random way to avoid symmetry.\n",
    "\n",
    "In the random initialize function the following value for epsilon is used:\n",
    "\n",
    "$$\\epsilon_{init} = \\frac{\\sqrt{6}}{\\sqrt{L_{in} + L_{out}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 3\n",
    "input_layer_size = X.shape[1]\n",
    "hidden_layer_size = 10000\n",
    "num_labels = y.shape[1]\n",
    "\n",
    "#choose a lambda\n",
    "lambda_ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps1 = math.sqrt(6)/math.sqrt(input_layer_size + hidden_layer_size)\n",
    "eps2 = math.sqrt(6)/math.sqrt(hidden_layer_size + num_labels)\n",
    "print('epsilon init 1 = ' + str(eps1))\n",
    "print('epsilon init 2 = ' + str(eps2))\n",
    "\n",
    "initial_Theta1 = cu.randInitializeWeights(input_layer_size, hidden_layer_size, eps1)\n",
    "initial_Theta2 = cu.randInitializeWeights(hidden_layer_size, num_labels, eps2)\n",
    "\n",
    "initial_nn_params = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-attachment",
   "metadata": {},
   "source": [
    "## learing the parameters with `scipy.optimize.minimize`\n",
    "next we will use the scipy.optimize.minimize function to minimize the randomly initialized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we initialize the minimize function options\n",
    "options = {'maxiter':50, 'disp':True}\n",
    "\n",
    "#creating a lambda function for the cost function\n",
    "costFunction = lambda p: cu.nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "\n",
    "# execute the optimization function\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution parameters\n",
    "nn_params = res.x\n",
    "\n",
    "#reshape nn_params to retrieve the seperate theta matrixes \n",
    "Theta1, Theta2 = cu.retrieveThetas(nn_params, input_layer_size, hidden_layer_size, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the theta's from nn_params\n",
    "Theta1, Theta2 = cu.retrieveThetas(nn_params, input_layer_size, hidden_layer_size, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-validity",
   "metadata": {},
   "source": [
    "## checking the cost function and the gradients\n",
    "next we will verify the outcome of the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "J, grads = cu.nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                      num_labels, X, y, lambda_)\n",
    "\n",
    "print('Cost at parameters: %.6f' % J)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-anniversary",
   "metadata": {},
   "source": [
    "## try the trained neural network on the training data\n",
    "the trained parameters are used to recognize the `micro PCB` in the images from the training data itself. This is a first test to check if the trained parameters can be correct. To get an actual idea of the accuracy of the neural network, check te next segment where the same predictions are done on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = utils.predict(Theta1, Theta2, X)\n",
    "y_vec = np.zeros(y.shape[0])\n",
    "\n",
    "\n",
    "for j in range(0,y.shape[0]):\n",
    "    y_vec[j] = np.where(y[j,:] == 1)[0][0]\n",
    "    print(\"y = \" + str(y_vec[j]) + \" <=> pred = \" + str(pred[j]))\n",
    "\n",
    "print('Training Set Accuracy: %f' % (np.mean(pred == y_vec) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-allah",
   "metadata": {},
   "source": [
    "## try the trained neural network on the test data\n",
    "The accuracy of the neural network is tested on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = utils.predict(Theta1, Theta2, X_test)\n",
    "y_test_vec = np.zeros(y_test.shape[0])\n",
    "\n",
    "for j in range(y_test_vec.shape[0]):\n",
    "    y_test_vec[j] = np.where(y_test[j,:] == 1)[0][0]\n",
    "    print(\"y = \" + str(y_test_vec[j]) + \" <=> pred = \" + str(pred_test[j]))\n",
    "\n",
    "print('Test Set Accuracy: %f' % (np.mean(pred_test == y_test_vec) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-rochester",
   "metadata": {},
   "source": [
    "## save/retrieve the theta values to/from mat file\n",
    "after minimizing theta it might be useful to save the values to a mat. The naming of the output files is `nnParameters_xL_y_lmZ.mat` where x is the number of layers and y is the number of nodes in the hidden layer(s) and Z is the chosen lambda.\n",
    "> do not forget to set the correct name/path for the output file as pleased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../nnParameters_norm/nnParameters_' + str(num_layers) +'L_' + str(hidden_layer_size) + '_lm' + str(lambda_) +'.mat'\n",
    "matcontent = {\"Theta1\": Theta1, \"Theta2\": Theta2}\n",
    "savemat(filename, matcontent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../nnParameters_norm/nnParameters_' + str(num_layers) +'L_' + str(hidden_layer_size) + '_lm' + str(lambda_) +'.mat'\n",
    "Thetas = loadmat(filename)\n",
    "Theta1, Theta2 = Thetas['Theta1'], Thetas['Theta2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-album",
   "metadata": {},
   "source": [
    "## Bias/Variance diagnosing\n",
    "In this section bias/variance is diagnosed by varying the `hidden layer size` as well as the `lambda` parameter. The error values are saved to a CSV named `diagnosing.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we initialize the minimize function options\n",
    "options = {'maxiter':200, 'disp':True}\n",
    "\n",
    "num_layers = 3\n",
    "input_layer_size = X.shape[1]\n",
    "num_labels = y.shape[1]\n",
    "\n",
    "#list of lambda values to be tested\n",
    "lambda_list = [0, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]\n",
    "\n",
    "#hidden layer size list\n",
    "HLS_list = [100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train = np.zeros((len(HLS_list), len(lambda_list)))\n",
    "error_test = np.zeros((len(HLS_list), len(lambda_list)))\n",
    "\n",
    "#with open('diagnosing3L.csv', 'a', newline='') as csvfile:\n",
    "#    fieldnames = ['HLS', 'lambda', 'trainError', 'testError']\n",
    "#    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#    writer.writeheader()\n",
    "    \n",
    "i = 0\n",
    "for HLS in HLS_list:\n",
    "    j = 0\n",
    "\n",
    "    eps1 = math.sqrt(6)/math.sqrt(input_layer_size + HLS)\n",
    "    eps2 = math.sqrt(6)/math.sqrt(HLS + num_labels)\n",
    "\n",
    "    initial_Theta1 = cu.randInitializeWeights(input_layer_size, HLS, eps1)\n",
    "    initial_Theta2 = cu.randInitializeWeights(HLS, num_labels, eps2)\n",
    "\n",
    "    initial_nn_params_start = np.concatenate([initial_Theta1.ravel(), initial_Theta2.ravel()], axis=0)\n",
    "\n",
    "    for lambda_val in lambda_list:\n",
    "        filename = '../nnParameters_norm/nnParameters_' + str(num_layers) +'L_' + str(HLS) + '_lm' + str(lambda_val) +'.mat'\n",
    "        if os.path.exists(filename):\n",
    "            print(\"loading existing thetas\")\n",
    "            Thetas = loadmat(filename)\n",
    "            Theta1, Theta2 = Thetas['Theta1'], Thetas['Theta2']\n",
    "            initial_nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()], axis=0)\n",
    "        else:\n",
    "            initial_nn_params =  initial_nn_params_start\n",
    "        \n",
    "        #creating a lambda function for the cost function\n",
    "        costFunction = lambda p: cu.nnCostFunction(p, input_layer_size,\n",
    "                                                HLS,\n",
    "                                                num_labels, X, y, lambda_val)\n",
    "\n",
    "        print(\"running optimize.minimize for HLS = \" + str(HLS) + \" and lambda = \" + str(lambda_val))\n",
    "        start = timer()\n",
    "\n",
    "        # execute the optimization function\n",
    "        res = optimize.minimize(costFunction,\n",
    "                                initial_nn_params,\n",
    "                                jac=True,\n",
    "                                method='TNC',\n",
    "                                options=options)\n",
    "\n",
    "        print(\"done optimizing, run time was \" + str(timer()-start))\n",
    "        #get the solution parameters\n",
    "        nn_params = res.x\n",
    "\n",
    "        #reshape nn_params to retrieve the seperate theta matrixes \n",
    "        Theta1 , Theta2 = cu.retrieveThetas(nn_params, input_layer_size, HLS, num_labels)\n",
    "\n",
    "        #calculate error on training set\n",
    "        error_train[i,j], _ = cu.nnCostFunction(nn_params, input_layer_size, HLS, num_labels, X, y, lambda_=0)\n",
    "        #calculate error on test set\n",
    "        error_test[i,j], _ = cu.nnCostFunction(nn_params, input_layer_size, HLS, num_labels, X_test, y_test, lambda_=0)\n",
    "        print(\"train error = \" + str(error_train[i,j]) + ' and test error = ' + str(error_test[i,j]))\n",
    "        \n",
    "        #print('saving theta values to MAT file')\n",
    "        print(\"writing\" + filename)\n",
    "        matcontent = {\"Theta1\": Theta1, \"Theta2\": Theta2}\n",
    "        savemat(filename, matcontent)\n",
    "\n",
    "        #set j for next loop\n",
    "        j += 1\n",
    "    #set i for next loop\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for HLS in HLS_list:\n",
    "    j = 0\n",
    "    for lambda_val in lambda_list:\n",
    "        filename = '../nnParameters_norm/nnParameters_' + str(num_layers) +'L_' + str(HLS) + '_lm' + str(lambda_val) +'.mat'\n",
    "        #Theta1, Theta2 = cu.thetasFromCSV(filePath, (HLS,(X.shape[1]+1)), (y.shape[1],(HLS+1))) \n",
    "        Thetas = loadmat(filename)\n",
    "        Theta1, Theta2 = Thetas['Theta1'], Thetas['Theta2']\n",
    "        \n",
    "        print('prediction accuracy for lambda = ' + str(lambda_val) + ' and HLS = ' + str(HLS))\n",
    "        \n",
    "        pred = utils.predict(Theta1, Theta2, X)\n",
    "\n",
    "        y_vec = np.zeros(y.shape[0])\n",
    "        for j in range(0,y.shape[0]):\n",
    "            y_vec[j] = np.where(y[j,:] == 1)[0][0]\n",
    "\n",
    "        print('Training Set Accuracy: %f' % (np.mean(pred == y_vec) * 100))\n",
    "        \n",
    "        pred_test = utils.predict(Theta1, Theta2, X_test)\n",
    "        y_test_vec = np.zeros(y_test.shape[0])\n",
    "\n",
    "        for j in range(y_test_vec.shape[0]):\n",
    "            y_test_vec[j] = np.where(y_test[j,:] == 1)[0][0]\n",
    "\n",
    "        print('Test Set Accuracy: %f' % (np.mean(pred_test == y_test_vec) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-consumer",
   "metadata": {},
   "source": [
    "### evaluating the trained models\n",
    "In this section the trained models are evaluated (cost calculated without regularisation) for the training set as well as the test set. These values are saved in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for HLS in HLS_list:\n",
    "    j = 0\n",
    "    for lambda_val in lambda_list:\n",
    "        filename = '../nnParameters_norm/nnParameters_' + str(num_layers) +'L_' + str(HLS) + '_lm' + str(lambda_val) +'.mat'\n",
    "        if os.path.exists(filename):\n",
    "            Thetas = loadmat(filename)\n",
    "            Theta1, Theta2 = Thetas['Theta1'], Thetas['Theta2']\n",
    "\n",
    "            nn_params = np.concatenate([Theta1.ravel(), Theta2.ravel()])\n",
    "            \n",
    "            #calculate error on training set\n",
    "            error_train, _ = cu.nnCostFunction(nn_params, input_layer_size, HLS, num_labels, X, y, lambda_=0)\n",
    "            #calculate error on test set\n",
    "            error_test, _ = cu.nnCostFunction(nn_params, input_layer_size, HLS, num_labels, X_test, y_test, lambda_=0)\n",
    "            print(\" writing train error = \" + str(error_train) + ' and test error = '\n",
    "                  + str(error_test) + ' for HLS ' + str(HLS) + ' and lambda ' + str(lambda_val))\n",
    "\n",
    "            with open('diagnosing3L_norm.csv', 'a', newline='') as csvfile:\n",
    "                fieldnames = ['HLS', 'lambda', 'trainError', 'testError']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                #save the values to CSV\n",
    "                writer.writerow({'HLS': HLS, 'lambda': lambda_val, 'trainError': error_train, 'testError': error_test})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-anchor",
   "metadata": {},
   "source": [
    "### plotting the evaluation data\n",
    "#### error vs amount of parameters (HLS) plot \n",
    "train error and test error are plotted for a chosen lambda with varying hidden layer size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-calcium",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_error = []\n",
    "test_error = []\n",
    "HLS_array = []\n",
    "\n",
    "with open('diagnosing3L_norm.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if float(row['lambda']) == 0:\n",
    "            train_error.append(float(row['trainError']))\n",
    "            test_error.append(float(row['testError']))\n",
    "            HLS_array.append(int(row['HLS']))\n",
    "#sort the array\n",
    "HLS_array, train_error, test_error = zip(*sorted(zip(HLS_array, train_error, test_error)))\n",
    "            \n",
    "pyplot.plot(HLS_array, train_error, 'b')\n",
    "pyplot.plot(HLS_array, test_error, 'r')\n",
    "pyplot.xscale('log')\n",
    "pyplot.axis([min(HLS_array)*0.8, max(HLS_array)*1.2, min(min(train_error),min(test_error))*0.8, max(max(train_error),max(test_error))*1.2])\n",
    "pyplot.legend([\"train\", \"test\"])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-metallic",
   "metadata": {},
   "source": [
    "#### error vs lambda plot\n",
    "train and test error are plotted vs lambda for a given hidden layer size. Be sure to set the desired HLS for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "HLS = 100\n",
    "\n",
    "train_error = []\n",
    "test_error = []\n",
    "lambda_array = []\n",
    "\n",
    "with open('diagnosing3L_norm.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if int(row['HLS']) == HLS and float(row['lambda']) != 0:\n",
    "            train_error.append(float(row['trainError']))\n",
    "            test_error.append(float(row['testError']))\n",
    "            lambda_array.append(float(row['lambda']))\n",
    "#sort the array\n",
    "lambda_array, train_error, test_error = zip(*sorted(zip(lambda_array, train_error, test_error)))\n",
    "\n",
    "pyplot.plot(lambda_array, train_error, 'b')\n",
    "pyplot.plot(lambda_array, test_error, 'r')\n",
    "pyplot.axis([min(lambda_array)*0.8, max(lambda_array)*1.2, min(min(train_error),min(test_error))*0.8, max(max(train_error),max(test_error))*1.2])\n",
    "pyplot.xscale('log')\n",
    "pyplot.legend([\"train\", \"test\"])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-interface",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
